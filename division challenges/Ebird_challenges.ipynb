{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG2JvBPSAixL",
        "outputId": "c93e6ecc-9810-4be2-bf48-a29e1805137d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Extracted 12 rows\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://birdcount.in/tag/ebird-monthly-challenge/page/3/?el_dbe_page'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Container for posts\n",
        "articles = soup.select('article.et_pb_post_extra')\n",
        "\n",
        "rows = []\n",
        "for art in articles:\n",
        "    # Title & link\n",
        "    h3 = art.select_one('h3.entry-title a')\n",
        "    title = h3.text.strip() if h3 else ''\n",
        "    link = h3['href'] if h3 else ''\n",
        "\n",
        "    # Content/summary\n",
        "    summary_tag = art.select_one('.post-data p')\n",
        "    content = summary_tag.text.strip() if summary_tag else ''\n",
        "\n",
        "    # Meta info\n",
        "    meta = art.select_one('p.post-meta')\n",
        "    date = comments = reading = ''\n",
        "    if meta:\n",
        "        spans = meta.find_all('span')\n",
        "        if len(spans) >= 3:\n",
        "            date = spans[0].text.strip()\n",
        "            comments = spans[1].text.strip()\n",
        "            reading = spans[2].text.strip()\n",
        "\n",
        "    # Image URL\n",
        "    img = art.select_one('.post-media img')\n",
        "    img_url = img['src'] if img else ''\n",
        "\n",
        "    rows.append([img_url, title, link, content, date, comments, reading])\n",
        "\n",
        "# Create DataFrame and export\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    'Image URL', 'Title', 'Article URL', 'Content', 'Date', 'Comments', 'Read Time'\n",
        "])\n",
        "df.to_excel('ebird_challenges_page3.xlsx', index=False)\n",
        "print(f\"‚úÖ Extracted {len(rows)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFpxq3bLEIuy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlh7HIKjGfZ1",
        "outputId": "c0486af5-bcc7-4c1a-9add-5aedddc2a4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fetching page 1: https://birdcount.in/tag/ebird-monthly-challenge/page/1/?el_dbe_page\n",
            "üîÑ Fetching page 2: https://birdcount.in/tag/ebird-monthly-challenge/page/2/?el_dbe_page\n",
            "üîÑ Fetching page 3: https://birdcount.in/tag/ebird-monthly-challenge/page/3/?el_dbe_page\n",
            "üîÑ Fetching page 4: https://birdcount.in/tag/ebird-monthly-challenge/page/4/?el_dbe_page\n",
            "üîÑ Fetching page 5: https://birdcount.in/tag/ebird-monthly-challenge/page/5/?el_dbe_page\n",
            "üîÑ Fetching page 6: https://birdcount.in/tag/ebird-monthly-challenge/page/6/?el_dbe_page\n",
            "üîÑ Fetching page 7: https://birdcount.in/tag/ebird-monthly-challenge/page/7/?el_dbe_page\n",
            "üîÑ Fetching page 8: https://birdcount.in/tag/ebird-monthly-challenge/page/8/?el_dbe_page\n",
            "üîÑ Fetching page 9: https://birdcount.in/tag/ebird-monthly-challenge/page/9/?el_dbe_page\n",
            "üîÑ Fetching page 10: https://birdcount.in/tag/ebird-monthly-challenge/page/10/?el_dbe_page\n",
            "üîÑ Fetching page 11: https://birdcount.in/tag/ebird-monthly-challenge/page/11/?el_dbe_page\n",
            "üîÑ Fetching page 12: https://birdcount.in/tag/ebird-monthly-challenge/page/12/?el_dbe_page\n",
            "üîÑ Fetching page 13: https://birdcount.in/tag/ebird-monthly-challenge/page/13/?el_dbe_page\n",
            "üîÑ Fetching page 14: https://birdcount.in/tag/ebird-monthly-challenge/page/14/?el_dbe_page\n",
            "üîÑ Fetching page 15: https://birdcount.in/tag/ebird-monthly-challenge/page/15/?el_dbe_page\n",
            "üîÑ Fetching page 16: https://birdcount.in/tag/ebird-monthly-challenge/page/16/?el_dbe_page\n",
            "üîÑ Fetching page 17: https://birdcount.in/tag/ebird-monthly-challenge/page/17/?el_dbe_page\n",
            "üîÑ Fetching page 18: https://birdcount.in/tag/ebird-monthly-challenge/page/18/?el_dbe_page\n",
            "üîÑ Fetching page 19: https://birdcount.in/tag/ebird-monthly-challenge/page/19/?el_dbe_page\n",
            "üîÑ Fetching page 20: https://birdcount.in/tag/ebird-monthly-challenge/page/20/?el_dbe_page\n",
            "üîÑ Fetching page 21: https://birdcount.in/tag/ebird-monthly-challenge/page/21/?el_dbe_page\n",
            "üîÑ Fetching page 22: https://birdcount.in/tag/ebird-monthly-challenge/page/22/?el_dbe_page\n",
            "üîÑ Fetching page 23: https://birdcount.in/tag/ebird-monthly-challenge/page/23/?el_dbe_page\n",
            "üõë No \"Load More\" button found on page 23. Done.\n",
            "‚úÖ Scraped 267 articles and saved to ebird_challenges_all.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "base_url = 'https://birdcount.in/tag/ebird-monthly-challenge'\n",
        "page = 1\n",
        "rows = []\n",
        "\n",
        "while True:\n",
        "    url = f'{base_url}/page/{page}/?el_dbe_page'\n",
        "    print(f'üîÑ Fetching page {page}: {url}')\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f'‚ùå Failed to load page {page}')\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.select('article.et_pb_post_extra')\n",
        "    if not articles:\n",
        "        print('‚úÖ No more articles found. Ending scrape.')\n",
        "        break\n",
        "\n",
        "    for art in articles:\n",
        "        h3 = art.select_one('h3.entry-title a')\n",
        "        title = h3.text.strip() if h3 else ''\n",
        "        link = h3['href'] if h3 else ''\n",
        "\n",
        "        summary_tag = art.select_one('.post-data p')\n",
        "        content = summary_tag.text.strip() if summary_tag else ''\n",
        "\n",
        "        meta = art.select_one('p.post-meta')\n",
        "        date = comments = reading = ''\n",
        "        if meta:\n",
        "            spans = meta.find_all('span')\n",
        "            if len(spans) >= 3:\n",
        "                date = spans[0].text.strip()\n",
        "                comments = spans[1].text.strip()\n",
        "                reading = spans[2].text.strip()\n",
        "\n",
        "        img = art.select_one('.post-media img')\n",
        "        img_url = img['src'] if img else ''\n",
        "\n",
        "        rows.append([img_url, title, link, content, date, comments, reading])\n",
        "\n",
        "    # Check if there's a \"Load More\" button\n",
        "    load_more = soup.select_one('a.el-load-more')\n",
        "    if not load_more:\n",
        "        print(f'üõë No \"Load More\" button found on page {page}. Done.')\n",
        "        break\n",
        "\n",
        "    page += 1\n",
        "    time.sleep(1)  # Be polite to the server\n",
        "\n",
        "# Save to Excel\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    'Image URL', 'Title', 'Article URL', 'Content', 'Date', 'Comments', 'Read Time'\n",
        "])\n",
        "df.to_excel('ebird_challenges_all.xlsx', index=False)\n",
        "print(f'‚úÖ Scraped {len(df)} articles and saved to ebird_challenges_all.xlsx')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohSdZE2au5Yk"
      },
      "source": [
        "# From other chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPGU7bjyT20-",
        "outputId": "c9ce8db7-7f26-4035-d330-d7e4573c35cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Fetching page 1: https://birdcount.in/tag/ebird-monthly-challenge/page/1/?el_dbe_page\n",
            "üîÑ Fetching page 2: https://birdcount.in/tag/ebird-monthly-challenge/page/2/?el_dbe_page\n",
            "üîÑ Fetching page 3: https://birdcount.in/tag/ebird-monthly-challenge/page/3/?el_dbe_page\n",
            "üîÑ Fetching page 4: https://birdcount.in/tag/ebird-monthly-challenge/page/4/?el_dbe_page\n",
            "üîÑ Fetching page 5: https://birdcount.in/tag/ebird-monthly-challenge/page/5/?el_dbe_page\n",
            "üîÑ Fetching page 6: https://birdcount.in/tag/ebird-monthly-challenge/page/6/?el_dbe_page\n",
            "üîÑ Fetching page 7: https://birdcount.in/tag/ebird-monthly-challenge/page/7/?el_dbe_page\n",
            "üîÑ Fetching page 8: https://birdcount.in/tag/ebird-monthly-challenge/page/8/?el_dbe_page\n",
            "üîÑ Fetching page 9: https://birdcount.in/tag/ebird-monthly-challenge/page/9/?el_dbe_page\n",
            "üîÑ Fetching page 10: https://birdcount.in/tag/ebird-monthly-challenge/page/10/?el_dbe_page\n",
            "üîÑ Fetching page 11: https://birdcount.in/tag/ebird-monthly-challenge/page/11/?el_dbe_page\n",
            "üîÑ Fetching page 12: https://birdcount.in/tag/ebird-monthly-challenge/page/12/?el_dbe_page\n",
            "üîÑ Fetching page 13: https://birdcount.in/tag/ebird-monthly-challenge/page/13/?el_dbe_page\n",
            "üîÑ Fetching page 14: https://birdcount.in/tag/ebird-monthly-challenge/page/14/?el_dbe_page\n",
            "üîÑ Fetching page 15: https://birdcount.in/tag/ebird-monthly-challenge/page/15/?el_dbe_page\n",
            "üîÑ Fetching page 16: https://birdcount.in/tag/ebird-monthly-challenge/page/16/?el_dbe_page\n",
            "üîÑ Fetching page 17: https://birdcount.in/tag/ebird-monthly-challenge/page/17/?el_dbe_page\n",
            "üîÑ Fetching page 18: https://birdcount.in/tag/ebird-monthly-challenge/page/18/?el_dbe_page\n",
            "üîÑ Fetching page 19: https://birdcount.in/tag/ebird-monthly-challenge/page/19/?el_dbe_page\n",
            "üîÑ Fetching page 20: https://birdcount.in/tag/ebird-monthly-challenge/page/20/?el_dbe_page\n",
            "üîÑ Fetching page 21: https://birdcount.in/tag/ebird-monthly-challenge/page/21/?el_dbe_page\n",
            "üîÑ Fetching page 22: https://birdcount.in/tag/ebird-monthly-challenge/page/22/?el_dbe_page\n",
            "üîÑ Fetching page 23: https://birdcount.in/tag/ebird-monthly-challenge/page/23/?el_dbe_page\n",
            "üõë No \"Load More\" button found on page 23. Done.\n",
            "‚úÖ Scraped 267 articles and saved to ebird_challenges_20250608.xlsx\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "base_url = 'https://birdcount.in/tag/ebird-monthly-challenge'\n",
        "page = 1\n",
        "rows = []\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "while True:\n",
        "    url = f'{base_url}/page/{page}/?el_dbe_page'\n",
        "    print(f'üîÑ Fetching page {page}: {url}')\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f'‚ùå Failed to load page {page} (status code {response.status_code})')\n",
        "            break\n",
        "    except RequestException as e:\n",
        "        print(f'‚ö†Ô∏è Request failed for page {page}: {e}')\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.select('article.et_pb_post_extra')\n",
        "    if not articles:\n",
        "        print('‚úÖ No more articles found. Ending scrape.')\n",
        "        break\n",
        "\n",
        "    for art in articles:\n",
        "        h3 = art.select_one('h3.entry-title a')\n",
        "        title = h3.text.strip() if h3 else ''\n",
        "        link = h3['href'] if h3 else ''\n",
        "\n",
        "        summary_tag = art.select_one('.post-data p')\n",
        "        content = summary_tag.text.strip() if summary_tag else ''\n",
        "\n",
        "        meta = art.select_one('p.post-meta')\n",
        "        date = comments = reading = ''\n",
        "        if meta:\n",
        "            spans = meta.find_all('span')\n",
        "            if len(spans) >= 3:\n",
        "                date = spans[0].text.strip()\n",
        "                comments = spans[1].text.strip()\n",
        "                reading = spans[2].text.strip()\n",
        "\n",
        "        img = art.select_one('.post-media img')\n",
        "        img_url = img['src'] if img else ''\n",
        "\n",
        "        rows.append([img_url, title, link, content, date, comments, reading])\n",
        "\n",
        "    # Check for \"Load More\" button\n",
        "    load_more = soup.select_one('a.el-load-more')\n",
        "    if not load_more:\n",
        "        print(f'üõë No \"Load More\" button found on page {page}. Done.')\n",
        "        break\n",
        "\n",
        "    page += 1\n",
        "    time.sleep(1)  # Be polite to the server\n",
        "\n",
        "# Save to Excel with timestamped filename\n",
        "df = pd.DataFrame(rows, columns=[\n",
        "    'Image URL', 'Title', 'Article URL', 'Content', 'Date', 'Comments', 'Read Time'\n",
        "])\n",
        "output_filename = f'ebird_challenges_{time.strftime(\"%Y%m%d\")}.xlsx'\n",
        "df.to_excel(output_filename, index=False)\n",
        "\n",
        "print(f'‚úÖ Scraped {len(df)} articles and saved to {output_filename}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfqrovSMT2yB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSrEd7fwT2vb",
        "outputId": "cc9f6bfa-460d-4d95-ddc8-3445daa7f7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting readability-lxml\n",
            "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.2.0)\n",
            "Collecting cssselect (from readability-lxml)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]->readability-lxml)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean, cssselect, readability-lxml\n",
            "Successfully installed cssselect-1.3.0 lxml_html_clean-0.4.2 readability-lxml-0.8.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install requests readability-lxml beautifulsoup4 lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uVHrihdwFZD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vfa7eU78wFSF",
        "outputId": "14da4ae4-3406-43e2-b48f-e5d6a04b3d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the URL: https://birdcount.in/apr2025-ebirders/\n",
            "\n",
            "üìù Extracted Main Content:\n",
            "\n",
            "üìÑ Title: April 2025 eBirders of the Month - Bird Count India\n",
            "\n",
            "April 2025 eBirders of the Month\n",
            "Before moving on to the results for the monthly challenges, here is a brief glimpse of birding in April by the numbers (with the previous month in brackets).\n",
            "No. of birders\n",
            ": 6,075 (7,324)\n",
            "Number of observations\n",
            ": 8.93 lakhs (12.52 lakhs)\n",
            "Number of lists (all types)\n",
            ": 54,400 (72,942)\n",
            "Number of species\n",
            ": 1,109 (1,119)\n",
            "Number of unique lists with media\n",
            ": 5,633 (7,135)\n",
            "The challenge for April was to upload a minimum of 20 checklists, with audio (rated) of at least 5 species. A total of\n",
            "44 eBirders\n",
            "successfully met this target!\n",
            "(Updated on 1 June¬† 2025)\n",
            ".\n",
            "Congratulations to all these birders!\n",
            "Ains Priestman, Ajay Sarvagnam, Amrit Raha, Anand Birdlife, Anand Singh, Aravind Am, Asim Giri, Bhaskar Mandal & Lakshmi Chatterjee, Biplab Banerjee, Chandu A, Chirag Munje, Dipankar Dev, Dr Bipasha David, Dr Mohammed Umer Sharieff, Gaja Mohanraj, Hariharan T V, Jayadev Menon,\n",
            "Kedar Champhekar\n",
            ", Kilson Kiragori, Kit Britten, Krishnamoorthy Muthirulan, Lakshmikant Neve, Madhavi Babtiwale, Maggie Geer, Mahmadanesh Khira, Munish Gowda, Neeraja V, Padma Gyalpo, Parthasarathi Chakrabarti, Pranad Patil, Rahul Wakare, Rajesh Radhakrishnan, Ramesh Shenai, Ranjeet Singh, Sandip Das, Sanjiv Khanna, Sharad Apte, Shilpa Gadgil, Shubham Giri, Soubhagya Mohanty, Sreekumar Chirukandoth, Uma Vaijnath, Vijaya Lakshmi, Vivek Sudhakaran.\n",
            "The above list does not include group accounts and those with no identifiable names.\n",
            "From these 44 names, one was drawn using a computer-generated random number. This person is\n",
            "Neeraja V\n",
            "who receives a copy of\n",
            "Women in the Wild: Stories of India‚Äôs Most Brilliant\n",
            "Women Wildlife Biologists\n",
            "by Anita Mani as a small gift in appreciation.\n",
            "Are you up to date with the eBird India challenge for\n",
            "May\n",
            "?\n",
            "Also, see here for the fresh set of\n",
            "yearlong challenges for 2025\n",
            "!\n",
            "Header Image\n",
            ": Rufous-winged Fulvetta\n",
            "Schoeniparus castaneceps\n",
            "¬© Manjula Desai / Macaulay Library\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from readability import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "def extract_main_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except RequestException as e:\n",
        "        return f\"‚ùå Failed to fetch page: {e}\"\n",
        "\n",
        "    # Use Readability to extract the main article content\n",
        "    doc = Document(response.text)\n",
        "    html_content = doc.summary()\n",
        "    title = doc.title()\n",
        "\n",
        "    # Optionally clean with BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    return f\"üìÑ Title: {title}\\n\\n{text}\"\n",
        "\n",
        "# üîΩ Input from user\n",
        "url = input(\"Enter the URL: \")\n",
        "main_content = extract_main_content(url)\n",
        "print(\"\\nüìù Extracted Main Content:\\n\")\n",
        "print(main_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PhjIYdVzgKc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH5Z05IvzgH4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NYveVhXzgC1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vzV6hrH1Uen"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oV0APvxx1Uv-",
        "outputId": "1aadfad5-5f5f-4be4-e9b6-b743fdc6f166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini API Response:\n",
            "\n",
            "{'candidates': [{'content': {'parts': [{'text': \"Here's the structured information extracted from the text:\\n\\n1.  **Number of birders:** 6,075 (for April, with 7,324 in the previous month)\\n2.  **Number of observations:** 8.93 lakhs (for April, with 12.52 lakhs in the previous month)\\n3.  **Number of lists:** 54,400 (for April, with 72,942 in the previous month)\\n4.  **Number of species:** 1,109 (for April, with 1,119 in the previous month)\\n5.  **Number of unique lists with media:** 5,633 (for April, with 7,135 in the previous month)\\n6.  **Names of birders (as a list):** ['Ains Priestman', 'Ajay Sarvagnam', 'Amrit Raha', 'Anand Birdlife', 'Anand Singh', 'Aravind Am', 'Asim Giri', 'Bhaskar Mandal & Lakshmi Chatterjee', 'Biplab Banerjee', 'Chandu A', 'Chirag Munje', 'Dipankar Dev', 'Dr Bipasha David', 'Dr Mohammed Umer Sharieff', 'Gaja Mohanraj', 'Hariharan T V', 'Jayadev Menon', 'Kedar Champhekar', 'Kilson Kiragori', 'Kit Britten', 'Krishnamoorthy Muthirulan', 'Lakshmikant Neve', 'Madhavi Babtiwale', 'Maggie Geer', 'Mahmadanesh Khira', 'Munish Gowda', 'Neeraja V', 'Padma Gyalpo', 'Parthasarathi Chakrabarti', 'Pranad Patil', 'Rahul Wakare', 'Rajesh Radhakrishnan', 'Ramesh Shenai', 'Ranjeet Singh', 'Sandip Das', 'Sanjiv Khanna', 'Sharad Apte', 'Shilpa Gadgil', 'Shubham Giri', 'Soubhagya Mohanty', 'Sreekumar Chirukandoth', 'Uma Vaijnath', 'Vijaya Lakshmi', 'Vivek Sudhakaran']\\n7.  **Winner's name:** Neeraja V\\n8.  **How was the winner chosen:** Drawn using a computer-generated random number.\\n9.  **Location of the challenge (if any):** India (based on Bird Count India and eBird India challenge)\\n10. **Upload requirements or conditions for completing the challenge:** Upload a minimum of 20 checklists, with audio (rated) of at least 5 species.\\n11. **Any tips or important points (list):**\\n    *   The list of birders does not include group accounts and those with no identifiable names.\\n    *   The winner receives a book as a gift.\\n    *   There are challenges for May and yearlong challenges for 2025.\\n12. **List of bird species mentioned (if any):** Rufous-winged Fulvetta\\n\"}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.041310519994966016}], 'usageMetadata': {'promptTokenCount': 776, 'candidatesTokenCount': 646, 'totalTokenCount': 1422, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 776}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 646}]}, 'modelVersion': 'gemini-2.0-flash', 'responseId': 'jZ9FaLy6MauShMIPhO_VgQs'}\n",
            "\n",
            "=== Extracted Structured Information ===\n",
            "\n",
            "Here's the structured information extracted from the text:\n",
            "\n",
            "1.  **Number of birders:** 6,075 (for April, with 7,324 in the previous month)\n",
            "2.  **Number of observations:** 8.93 lakhs (for April, with 12.52 lakhs in the previous month)\n",
            "3.  **Number of lists:** 54,400 (for April, with 72,942 in the previous month)\n",
            "4.  **Number of species:** 1,109 (for April, with 1,119 in the previous month)\n",
            "5.  **Number of unique lists with media:** 5,633 (for April, with 7,135 in the previous month)\n",
            "6.  **Names of birders (as a list):** ['Ains Priestman', 'Ajay Sarvagnam', 'Amrit Raha', 'Anand Birdlife', 'Anand Singh', 'Aravind Am', 'Asim Giri', 'Bhaskar Mandal & Lakshmi Chatterjee', 'Biplab Banerjee', 'Chandu A', 'Chirag Munje', 'Dipankar Dev', 'Dr Bipasha David', 'Dr Mohammed Umer Sharieff', 'Gaja Mohanraj', 'Hariharan T V', 'Jayadev Menon', 'Kedar Champhekar', 'Kilson Kiragori', 'Kit Britten', 'Krishnamoorthy Muthirulan', 'Lakshmikant Neve', 'Madhavi Babtiwale', 'Maggie Geer', 'Mahmadanesh Khira', 'Munish Gowda', 'Neeraja V', 'Padma Gyalpo', 'Parthasarathi Chakrabarti', 'Pranad Patil', 'Rahul Wakare', 'Rajesh Radhakrishnan', 'Ramesh Shenai', 'Ranjeet Singh', 'Sandip Das', 'Sanjiv Khanna', 'Sharad Apte', 'Shilpa Gadgil', 'Shubham Giri', 'Soubhagya Mohanty', 'Sreekumar Chirukandoth', 'Uma Vaijnath', 'Vijaya Lakshmi', 'Vivek Sudhakaran']\n",
            "7.  **Winner's name:** Neeraja V\n",
            "8.  **How was the winner chosen:** Drawn using a computer-generated random number.\n",
            "9.  **Location of the challenge (if any):** India (based on Bird Count India and eBird India challenge)\n",
            "10. **Upload requirements or conditions for completing the challenge:** Upload a minimum of 20 checklists, with audio (rated) of at least 5 species.\n",
            "11. **Any tips or important points (list):**\n",
            "    *   The list of birders does not include group accounts and those with no identifiable names.\n",
            "    *   The winner receives a book as a gift.\n",
            "    *   There are challenges for May and yearlong challenges for 2025.\n",
            "12. **List of bird species mentioned (if any):** Rufous-winged Fulvetta\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def send_to_gemini(api_key: str, page_content: str):\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Build the prompt with instructions + your scraped content\n",
        "    prompt_text = f\"\"\"\n",
        "Extract structured information from the text below about a birding challenge. For each item, provide the value or list if available, or 'Not found' if missing.\n",
        "\n",
        "1. Number of birders\n",
        "2. Number of observations\n",
        "3. Number of lists\n",
        "4.Number of species\n",
        "5.Number of unique lists with media\n",
        "6.Names of birders (as a list)\n",
        "7.Winner's name\n",
        "8.How was the winner chosen\n",
        "9.Location of the challenge (if any)\n",
        "10. Upload requirements or conditions for completing the challenge\n",
        "11. Any tips or important points (list)\n",
        "12. List of bird species mentioned (if any)\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{page_content}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": prompt_text.strip()\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def pretty_print_gemini_response(response_json):\n",
        "    try:\n",
        "        # Extract the first candidate's first part's text\n",
        "        candidates = response_json.get(\"candidates\", [])\n",
        "        if not candidates:\n",
        "            print(\"No candidates found in response.\")\n",
        "            return\n",
        "\n",
        "        content = candidates[0].get(\"content\", {})\n",
        "        parts = content.get(\"parts\", [])\n",
        "        if not parts:\n",
        "            print(\"No parts found in content.\")\n",
        "            return\n",
        "\n",
        "        text = parts[0].get(\"text\", \"\")\n",
        "\n",
        "        # Print clean output\n",
        "        print(\"\\n=== Extracted Structured Information ===\\n\")\n",
        "        print(text.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while parsing response: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace this with your actual API key\n",
        "    API_KEY = \"YOUR_API_KEY\"\n",
        "\n",
        "    # Paste the scraped page content here as a multiline string\n",
        "    page_content = \"\"\"\n",
        "Enter the URL: https://birdcount.in/apr2025-ebirders/\n",
        "\n",
        "April 2025 eBirders of the Month - Bird Count India\n",
        "\n",
        "April 2025 eBirders of the Month\n",
        "Before moving on to the results for the monthly challenges, here is a brief glimpse of birding in April by the numbers (with the previous month in brackets).\n",
        "No. of birders\n",
        ": 6,075 (7,324)\n",
        "Number of observations\n",
        ": 8.93 lakhs (12.52 lakhs)\n",
        "Number of lists (all types)\n",
        ": 54,400 (72,942)\n",
        "Number of species\n",
        ": 1,109 (1,119)\n",
        "Number of unique lists with media\n",
        ": 5,633 (7,135)\n",
        "The challenge for April was to upload a minimum of 20 checklists, with audio (rated) of at least 5 species. A total of\n",
        "44 eBirders\n",
        "successfully met this target!\n",
        "(Updated on 1 June  2025)\n",
        ".\n",
        "Congratulations to all these birders!\n",
        "Ains Priestman, Ajay Sarvagnam, Amrit Raha, Anand Birdlife, Anand Singh, Aravind Am, Asim Giri, Bhaskar Mandal & Lakshmi Chatterjee, Biplab Banerjee, Chandu A, Chirag Munje, Dipankar Dev, Dr Bipasha David, Dr Mohammed Umer Sharieff, Gaja Mohanraj, Hariharan T V, Jayadev Menon,\n",
        "Kedar Champhekar\n",
        ", Kilson Kiragori, Kit Britten, Krishnamoorthy Muthirulan, Lakshmikant Neve, Madhavi Babtiwale, Maggie Geer, Mahmadanesh Khira, Munish Gowda, Neeraja V, Padma Gyalpo, Parthasarathi Chakrabarti, Pranad Patil, Rahul Wakare, Rajesh Radhakrishnan, Ramesh Shenai, Ranjeet Singh, Sandip Das, Sanjiv Khanna, Sharad Apte, Shilpa Gadgil, Shubham Giri, Soubhagya Mohanty, Sreekumar Chirukandoth, Uma Vaijnath, Vijaya Lakshmi, Vivek Sudhakaran.\n",
        "The above list does not include group accounts and those with no identifiable names.\n",
        "From these 44 names, one was drawn using a computer-generated random number. This person is\n",
        "Neeraja V\n",
        "who receives a copy of\n",
        "Women in the Wild: Stories of India‚Äôs Most Brilliant\n",
        "Women Wildlife Biologists\n",
        "by Anita Mani as a small gift in appreciation.\n",
        "Are you up to date with the eBird India challenge for\n",
        "May\n",
        "?\n",
        "Also, see here for the fresh set of\n",
        "yearlong challenges for 2025\n",
        "!\n",
        "Header Image\n",
        ": Rufous-winged Fulvetta\n",
        "Schoeniparus castaneceps\n",
        "¬© Manjula Desai / Macaulay Library\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = send_to_gemini('AIzaSyBMUGzMz7SxlBvJ2KdsQE0Ovez2kHJo3No', page_content)\n",
        "        print(\"Gemini API Response:\\n\")\n",
        "        print(response)\n",
        "        pretty_print_gemini_response(response)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to get response from Gemini API:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBJshFt41Ub7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXl6T2TQ9usB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVpf4FNz9upj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK7847u_OtLl",
        "outputId": "c98a4103-174c-403d-ae20-79b4c3837b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the URL: https://birdcount.in/jun14-ebirders-2/\n",
            "\n",
            "üìù Extracted Main Content:\n",
            "\n",
            "üìÑ Title: April 2014 eBirders of the Month - Bird Count India\n",
            "\n",
            "April 2014 eBirders of the Month\n",
            "April has come and gone, and it‚Äôs time to crunch the numbers and see who has been able to meet the\n",
            "eBirding Challenge for the month\n",
            ", which was to upload at least 20 complete lists\n",
            "from India during the month.\n",
            "During April,\n",
            "194\n",
            "eBird users uploaded\n",
            "1,237\n",
            "lists of all types, which together accounted for\n",
            "26,481\n",
            "records.\n",
            "In all,\n",
            "17\n",
            "eBirders met or exceeded the April target. They are:\n",
            "Aidan & Savio Fonseca\n",
            "Anish Aravind\n",
            "Arya Vinod\n",
            "Bela Arora\n",
            "Dhananjai Mohan\n",
            "Ganeshwar S V\n",
            "Ishan Sadwelkar\n",
            "Manish Kumar\n",
            "Manju Sinha\n",
            "Michael Emenaker\n",
            "Panchapakesan Jeganathan\n",
            "Premchand Reghuvaran\n",
            "Pronoy Baidya\n",
            "Raja Simma Pandiyan\n",
            "Raman Kumar\n",
            "Shivaprakash Adavanne\n",
            "Suhel Quader\n",
            "Many congratulations to each of these\n",
            "eBirders of the Month\n",
            "for April 2014!\n",
            "And, as promised, one of these 17 has been chosen using a computer-generated random number to receive a small gift, and that person is\n",
            "Ganeshwar S V\n",
            "who receives a copy of Simon Barnes‚Äôs\n",
            "How to be a (Bad) Birdwatcher: To the Greater Glory of Life\n",
            ". (You can read a\n",
            "review here\n",
            ".)\n",
            "Congratulations once more, and see you at the\n",
            "May eBirding Challenge\n",
            "!\n",
            "Header Image\n",
            ":\n",
            "Blyth‚Äôs Tragopan\n",
            "Tragopan blythii\n",
            "¬©\n",
            "Rofikul Islam/ Macaulay Library\n",
            "Gemini API Response:\n",
            "\n",
            "{'candidates': [{'content': {'parts': [{'text': \"Here's the extracted structured information from the provided text:\\n\\n1.  **Number of birders:** 17\\n2.  **Number of observations:** 26,481\\n3.  **Number of lists:** 1,237 total, 20 were required for the challenge\\n4.  **Number of species:** Not found\\n5.  **Number of unique lists with media:** Not found\\n6.  **Names of birders (as a list):**\\n    *   Aidan & Savio Fonseca\\n    *   Anish Aravind\\n    *   Arya Vinod\\n    *   Bela Arora\\n    *   Dhananjai Mohan\\n    *   Ganeshwar S V\\n    *   Ishan Sadwelkar\\n    *   Manish Kumar\\n    *   Manju Sinha\\n    *   Michael Emenaker\\n    *   Panchapakesan Jeganathan\\n    *   Premchand Reghuvaran\\n    *   Pronoy Baidya\\n    *   Raja Simma Pandiyan\\n    *   Raman Kumar\\n    *   Shivaprakash Adavanne\\n    *   Suhel Quader\\n7.  **Winner's name:** Ganeshwar S V\\n8.  **How was the winner chosen:** Using a computer-generated random number\\n9.  **Location of the challenge (if any):** India\\n10. **Upload requirements or conditions for completing the challenge:** Upload at least 20 complete lists.\\n11. **Any tips or important points (list):** Not found\\n12. **List of bird species mentioned (if any):** Blyth‚Äôs Tragopan (Tragopan blythii)\\n\"}], 'role': 'model'}, 'finishReason': 'STOP', 'citationMetadata': {'citationSources': [{'startIndex': 347, 'endIndex': 741, 'uri': 'https://birdcount.in/april-ebirders-of-the-month/'}]}, 'avgLogprobs': -0.05597604357677957}], 'usageMetadata': {'promptTokenCount': 536, 'candidatesTokenCount': 368, 'totalTokenCount': 904, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 536}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 368}]}, 'modelVersion': 'gemini-2.0-flash', 'responseId': 'trFFaJ-SMouvgLUPlMrL2Qk'}\n",
            "\n",
            "=== Extracted Structured Information ===\n",
            "\n",
            "Here's the extracted structured information from the provided text:\n",
            "\n",
            "1.  **Number of birders:** 17\n",
            "2.  **Number of observations:** 26,481\n",
            "3.  **Number of lists:** 1,237 total, 20 were required for the challenge\n",
            "4.  **Number of species:** Not found\n",
            "5.  **Number of unique lists with media:** Not found\n",
            "6.  **Names of birders (as a list):**\n",
            "    *   Aidan & Savio Fonseca\n",
            "    *   Anish Aravind\n",
            "    *   Arya Vinod\n",
            "    *   Bela Arora\n",
            "    *   Dhananjai Mohan\n",
            "    *   Ganeshwar S V\n",
            "    *   Ishan Sadwelkar\n",
            "    *   Manish Kumar\n",
            "    *   Manju Sinha\n",
            "    *   Michael Emenaker\n",
            "    *   Panchapakesan Jeganathan\n",
            "    *   Premchand Reghuvaran\n",
            "    *   Pronoy Baidya\n",
            "    *   Raja Simma Pandiyan\n",
            "    *   Raman Kumar\n",
            "    *   Shivaprakash Adavanne\n",
            "    *   Suhel Quader\n",
            "7.  **Winner's name:** Ganeshwar S V\n",
            "8.  **How was the winner chosen:** Using a computer-generated random number\n",
            "9.  **Location of the challenge (if any):** India\n",
            "10. **Upload requirements or conditions for completing the challenge:** Upload at least 20 complete lists.\n",
            "11. **Any tips or important points (list):** Not found\n",
            "12. **List of bird species mentioned (if any):** Blyth‚Äôs Tragopan (Tragopan blythii)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "import requests\n",
        "from readability import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "def extract_main_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except RequestException as e:\n",
        "        return f\"‚ùå Failed to fetch page: {e}\"\n",
        "\n",
        "    # Use Readability to extract the main article content\n",
        "    doc = Document(response.text)\n",
        "    html_content = doc.summary()\n",
        "    title = doc.title()\n",
        "\n",
        "    # Optionally clean with BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    return f\"üìÑ Title: {title}\\n\\n{text}\"\n",
        "\n",
        "def send_to_gemini(api_key: str, page_content: str):\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Build the prompt with instructions + your scraped content\n",
        "    prompt_text = f\"\"\"\n",
        "Extract structured information from the text below about a birding challenge. For each item, provide the value or list if available, or 'Not found' if missing.\n",
        "\n",
        "1. Number of birders\n",
        "2. Number of observations\n",
        "3. Number of lists\n",
        "4.Number of species\n",
        "5.Number of unique lists with media\n",
        "6.Names of birders (as a list)\n",
        "7.Winner's name\n",
        "8.How was the winner chosen\n",
        "9.Location of the challenge (if any)\n",
        "10. Upload requirements or conditions for completing the challenge\n",
        "11. Any tips or important points (list)\n",
        "12. List of bird species mentioned (if any)\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{page_content}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": prompt_text.strip()\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def pretty_print_gemini_response(response_json):\n",
        "    try:\n",
        "        # Extract the first candidate's first part's text\n",
        "        candidates = response_json.get(\"candidates\", [])\n",
        "        if not candidates:\n",
        "            print(\"No candidates found in response.\")\n",
        "            return\n",
        "\n",
        "        content = candidates[0].get(\"content\", {})\n",
        "        parts = content.get(\"parts\", [])\n",
        "        if not parts:\n",
        "            print(\"No parts found in content.\")\n",
        "            return\n",
        "\n",
        "        text = parts[0].get(\"text\", \"\")\n",
        "\n",
        "        # Print clean output\n",
        "        print(\"\\n=== Extracted Structured Information ===\\n\")\n",
        "        print(text.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while parsing response: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = input(\"Enter the URL: \")\n",
        "    main_content = extract_main_content(url)\n",
        "    API_KEY = \"AIzaSyBMUGzMz7SxlBvJ2KdsQE0Ovez2kHJo3No\"\n",
        "    page_content = main_content\n",
        "\n",
        "    try:\n",
        "        response = send_to_gemini(API_KEY, page_content)\n",
        "        print(\"Gemini API Response:\\n\")\n",
        "        print(response)\n",
        "        pretty_print_gemini_response(response)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to get response from Gemini API:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6-20k8DVF5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRUZ7qRvVF2o",
        "outputId": "dabe4371-9c84-42fb-e5ee-eaacc2326e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting readability-lxml\n",
            "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from readability-lxml) (5.2.0)\n",
            "Collecting cssselect (from readability-lxml)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]->readability-lxml)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Downloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean, cssselect, readability-lxml\n",
            "Successfully installed cssselect-1.3.0 lxml_html_clean-0.4.2 readability-lxml-0.8.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install requests readability-lxml beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HTDOApxVFzg",
        "outputId": "b7c61876-3fca-4ed0-9f0e-c5a9b5bc9c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîó Processing URL 1: https://birdcount.in/jun25-challenge/\n",
            "üì• Gemini Response:\n",
            "\n",
            "=== Extracted Structured Information ===\n",
            "\n",
            "Here's the structured information extracted from the text:\n",
            "\n",
            "1.  **Number of birders:** Not found\n",
            "2.  **Number of observations:** Not found\n",
            "3.  **Number of lists:** 25 eligible checklists\n",
            "4.  **Number of species:** Not found\n",
            "5.  **Number of unique lists with media:** Not found\n",
            "6.  **Names of birders (as a list):** Not found\n",
            "7.  **Winner's name:** Not found\n",
            "8.  **How was the winner chosen:** Not found\n",
            "9.  **Location of the challenge (if any):** India\n",
            "10. **Upload requirements or conditions for completing the challenge:**\n",
            "    *   25 eligible checklists\n",
            "    *   At least five checklists must contain a brood-parasitic cuckoo\n",
            "    *   Eligible checklists must report ALL species seen/heard (and are marked 'complete')\n",
            "    *   Include counts for all species numbers (no 'X' entries)\n",
            "    *   Checklists must be at least 15 minutes or longer in duration\n",
            "    *   Upload all lists by 10 July 2025\n",
            "\n",
            "11. **Any tips or important points (list):**\n",
            "    *   Encouragement to add sounds, photos, and behavioral comments to checklists.\n",
            "    *   Use appropriate breeding behavior codes when observing or documenting breeding birds.\n",
            "    *   Maintain a suitable distance from the birds.\n",
            "    *   Avoid playback of bird calls.\n",
            "    *   Be mindful of sensitive habitats, especially nesting areas.\n",
            "    *   Consider delaying the upload of comments or photographs if they may attract unwanted attention and disturb nesting birds.\n",
            "\n",
            "12. **List of bird species mentioned (if any):**\n",
            "    *   Himalayan Cuckoo\n",
            "    *   Common Hawk-Cuckoo\n",
            "    *   Violet Cuckoo (Chrysococcyx xanthorhynchus)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "from readability import Document\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "def get_api_key():\n",
        "  return \"AIzaSyBMUGzMz7SxlBvJ2KdsQE0Ovez2kHJo3No\"\n",
        "\n",
        "def get_prompt_text():\n",
        "  return (f\"\"\"\n",
        "Extract structured information from the text below about a birding challenge. For each item, provide the value or list if available, or 'Not found' if missing.\n",
        "\n",
        "1. Number of birders\n",
        "2. Number of observations\n",
        "3. Number of lists\n",
        "4. Number of species\n",
        "5. Number of unique lists with media\n",
        "6. Names of birders (as a list)\n",
        "7. Winner's name\n",
        "8. How was the winner chosen\n",
        "9. Location of the challenge (if any)\n",
        "10. Upload requirements or conditions for completing the challenge\n",
        "11. Any tips or important points (list)\n",
        "12. List of bird species mentioned (if any)\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{page_content}\\\"\\\"\\\"\n",
        "\"\"\")\n",
        "def extract_main_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except RequestException as e:\n",
        "        return f\"‚ùå Failed to fetch page: {e}\"\n",
        "\n",
        "    # Use Readability to extract the main article content\n",
        "    doc = Document(response.text)\n",
        "    html_content = doc.summary()\n",
        "    title = doc.title()\n",
        "\n",
        "    # Optionally clean with BeautifulSoup\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text = soup.get_text(separator='\\n', strip=True)\n",
        "\n",
        "    return f\"üìÑ Title: {title}\\n\\n{text}\"\n",
        "\n",
        "def send_to_gemini(api_key: str, page_content: str):\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Build the prompt with instructions + your scraped content\n",
        "    prompt_text = get_prompt_text()\n",
        "\n",
        "    data = {\n",
        "        \"contents\": [\n",
        "            {\n",
        "                \"parts\": [\n",
        "                    {\n",
        "                        \"text\": prompt_text.strip()\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "def pretty_print_gemini_response(response_json):\n",
        "    try:\n",
        "        # Extract the first candidate's first part's text\n",
        "        candidates = response_json.get(\"candidates\", [])\n",
        "        if not candidates:\n",
        "            print(\"No candidates found in response.\")\n",
        "            return\n",
        "\n",
        "        content = candidates[0].get(\"content\", {})\n",
        "        parts = content.get(\"parts\", [])\n",
        "        if not parts:\n",
        "            print(\"No parts found in content.\")\n",
        "            return\n",
        "\n",
        "        text = parts[0].get(\"text\", \"\")\n",
        "\n",
        "        # Print clean output\n",
        "        print(\"\\n=== Extracted Structured Information ===\\n\")\n",
        "        print(text.strip())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while parsing response: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_excel(\"/content/ebird_challenges_20250608.xlsx\")\n",
        "    urls = df[\"Article URL\"].dropna().tolist()\n",
        "    urls = urls[:1]  # You can change 5 to any other stopper\n",
        "    API_KEY = get_api_key()\n",
        "\n",
        "    for idx, url in enumerate(urls, start=1):\n",
        "        print(f\"\\nüîó Processing URL {idx}: {url}\")\n",
        "        try:\n",
        "            page_content = extract_main_content(url)\n",
        "            response = send_to_gemini(API_KEY, page_content)\n",
        "\n",
        "            print(\"üì• Gemini Response:\")\n",
        "            pretty_print_gemini_response(response)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to process URL {idx}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
